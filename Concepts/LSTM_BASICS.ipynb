{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T14:52:21.026749Z",
     "start_time": "2020-12-21T14:52:21.021914Z"
    }
   },
   "outputs": [],
   "source": [
    "# this blog is all you need to about LSTM https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "# Most of the stuff is taken from this blog https://datascience-enthusiast.com/DL/Building_a_Recurrent_Neural_Network-Step_by_Step_v1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHY LSTM \n",
    "If we think from RNN perspective wkt RNN has only 2 parts, input and hidden state. RNNs have feedback loops in the recurrent layer. This lets them maintain information in 'memory' over time. But, it can be difficult to train standard RNNs to solve problems that require learning long-term temporal dependencies. This is because the gradient of the loss function decays exponentially with time (called the vanishing gradient problem). \n",
    "\n",
    "LSTM networks are a type of RNN that uses special units in addition to standard units. LSTM units include a 'memory cell' that can maintain information in memory for long periods of time. A set of gates is used to control when information enters the memory, when it's output, and when it's forgotten. This architecture lets them learn longer-term dependencies.LSTM Cell state could be compared to resnet skip connection because the information can flow longer time \n",
    "\n",
    "GRUs are similar to LSTMs, but use a simplified structure. They also use a set of gates to control the flow of information, but they don't use separate memory cells, and they use fewer gates.\n",
    "\n",
    "##  LSTM Different parts\n",
    "LSTM could be summarised as having the these parts\n",
    "1. Context vector C\n",
    "2. Forget gate to remove values from the Context Vector\n",
    "3. Input gate which controls what new information to be added to the Context vector C\n",
    "4. Output gate to finally filter the output from Context vector using the hidden state and input\n",
    "\n",
    "#### NOTE: We use sigmoid as a controlling gate and tanh everywhere else "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T14:05:08.719420Z",
     "start_time": "2020-12-21T14:05:08.589142Z"
    }
   },
   "source": [
    "![LSTM Cell](./images/lstm_cell.png)\n",
    "\n",
    "#### Note : \n",
    "    In the figure the forget gate , update gate and the output gate are actually a sigmoid function\n",
    "    Input is Xt\n",
    "    Hidden state is at\n",
    "    Cell state is ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Context Vector C : <br>\n",
    "    So context vectors are used to store long term context. We need to update the context vector using gates, The term gates here are in the context of soft gates and the parameters are learned overtime. We remove old information using a forget gate. We add new information using update/input gate. Since the cell state could hold values from any range we don't directly use it as output. Instead we pass it through a tanh gate and then use one output gate to finally give output. The forget gate output gets multiplied by the cell state values to change cell state.The input gate output is added to the cell state(why not multiplied??)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The Forgate Gate : <br>\n",
    "    The forgate gate takes an input which is a concatenation of input(xt) and the hidden state(at).This becomes a input of a hidden layer with weight Wf and the output is passed through a sigmoid gate(forget gate). We can think of this neural network as a dedicated network which will learn to forget things using the back propagation and thus the context vector will get updated.\n",
    "    \n",
    "    Γ⟨t⟩f=σ(Wf[a⟨t−1⟩,x⟨t⟩]+bf)          \n",
    "    \n",
    "    Here,  Wf  are weights that govern the forget gate's behavior. We concatenate  [a⟨t−1⟩,x⟨t⟩]  and multiply by  Wf . The equation above results in a vector  Γ⟨t⟩f  with values between 0 and 1. This forget gate vector will be multiplied element-wise by the previous cell state  c⟨t−1⟩ . So if one of the values of  Γ⟨t⟩f  is 0 (or close to 0) then it means that the LSTM should remove that piece of information (e.g. the singular subject) in the corresponding component of  c⟨t−1⟩ . If one of the values is 1, then it will keep the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The Update/Input Gate : <br>\n",
    "\n",
    "    The Update gate takes an input which is a concatenation of input(xt) and the hidden state(at). If you notice the tanh part of the input gate is actually one RNN cell. This Rnn cell. This could be thought of as a short term memory element. This will be our new candidate to update the cell state will.(Update cell state with the current context). But instead of directly sending it to the Cell state we use one more neural network with sigmoid. This could be thought of as a neural network controlling the information flow. I would like to add the current context to the cell state, also I have one adviser who is telling me which values i should add.Thus we can add much useful/filtered context to the context vector\n",
    "    \n",
    "    Γ⟨t⟩u=σ(Wu[a⟨t−1⟩,x{t}]+bu)\n",
    "    \n",
    "    c~⟨t⟩=tanh(Wc[a⟨t−1⟩,x⟨t⟩]+bc)\n",
    "    \n",
    "    Now we can update the cell state \n",
    "    \n",
    "    c⟨t⟩=Γ⟨t⟩f∗c⟨t−1⟩+Γ⟨t⟩u∗c~⟨t⟩\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The Output Gate : <br>\n",
    "    Since we know that cell state could have any range of values(because we have one addition operation).The cell state naturally has long term context over multiple time-steps, We would use this to get our final output. So we let it pass though a tanh gate which spits out values between -1 to 1 and then. This output is then multiplied with one more sigmoid output gate(input is a concatenation of input(xt) and the hidden state(at)). So more neural network(hidden layer) dedicated for the output. This output is hidden state for next time-step. Also this could be the output of the lstm depending on the problem in hand.\n",
    "    \n",
    "    Γ⟨t⟩o=σ(Wo[a⟨t−1⟩,x⟨t⟩]+bo)\n",
    "    a⟨t⟩=Γ⟨t⟩o∗tanh(c⟨t⟩)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Things to remember:\n",
    "    1. So LSTM has 3 sets of learnable parameters, the forgate gate weights Wf, The input gate/Update gate weights Wu and the output weights Wo\n",
    "    2. Having a cell state helps to keep context for long time. The Cell state only gets updated by linear operations.\n",
    "    3. GRU is a simplified version of LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
