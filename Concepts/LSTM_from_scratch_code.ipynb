{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM FORWARD PROPAGATION CODE WITH EQUATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits:<br>\n",
    "https://datascience-enthusiast.com/DL/Building_a_Recurrent_Neural_Network-Step_by_Step_v1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T06:48:10.189859Z",
     "start_time": "2020-12-21T06:48:10.077563Z"
    }
   },
   "source": [
    "![LSTM Cell](./images/lstm_cell.png)\n",
    "![LSTM Cell Forward](./images/lstm_cell_forward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM CELL FORWARD PROPAGATION FOR ONE CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T09:04:04.058524Z",
     "start_time": "2020-12-21T09:04:04.056614Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T09:04:04.474828Z",
     "start_time": "2020-12-21T09:04:04.471597Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T09:04:04.847229Z",
     "start_time": "2020-12-21T09:04:04.843086Z"
    }
   },
   "outputs": [],
   "source": [
    "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
    "    \"\"\"\n",
    "    Impliments a single forward step of LSTM-cell as described in the fig above\n",
    "    \n",
    "    Arguments:\n",
    "    xt -- input data at timesteps \"t\", numpy array of shape(n_x,m)\n",
    "    a_prev -- hidden state at timestep \"t-1\", numpy array of shape(n_a,m)\n",
    "    c_prev -- Memory state at timestep \"t-1\", numpy array of shape(n_a,m)\n",
    "    \n",
    "    parameters -- python dict containing:\n",
    "                    Wf -- Weights matrix of the forgate gate, numpy array of shape(n_a,n_a+n_x)\n",
    "                    bf -- Bias of the forgate gate, numpy array of shape (n_a,1)\n",
    "                    Wi -- Weights matrix of the update gate, numpy array of shape(n_a,n_a+n_x)\n",
    "                    bi -- Bias of the update gate, numpy array of shape (n_a,1)\n",
    "                    Wc -- Weights matrix of the first \"tanh\", numpy array of shape(n_a,n_a+n_x)\n",
    "                    bc -- Bias of the first \"tanh\", numpy array of shape (n_a,1)\n",
    "                    Wo -- Weights matrix of the output gate, numpy array of shape(n_a,n_a+n_x)\n",
    "                    bo -- Bias of the output gate, numpy array of shape (n_a,1)\n",
    "                    Wf -- Weights matrix relating the hidden state to the output, numpy array of shape(n_y,n_a)\n",
    "                    bf -- Bias relating the hidden state to the output, numpy array of shape (n_y,1)\n",
    "    Returns:\n",
    "    a_next -- next hidden state, of shape(n_a,m)\n",
    "    c_next -- next memory state, of shape (n_a,m)\n",
    "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y,m)\n",
    "    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)\n",
    "    \n",
    "    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),\n",
    "          c stands for the memory value\n",
    "    \"\"\"\n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wf = parameters[\"Wf\"]\n",
    "    bf = parameters[\"bf\"]\n",
    "    Wi = parameters[\"Wi\"]\n",
    "    bi = parameters[\"bi\"]\n",
    "    Wc = parameters[\"Wc\"]\n",
    "    bc = parameters[\"bc\"]\n",
    "    Wo = parameters[\"Wo\"]\n",
    "    bo = parameters[\"bo\"]\n",
    "    Wy = parameters[\"Wy\"]\n",
    "    by = parameters[\"by\"]\n",
    "\n",
    "    # Retrieve dimensions from shapes of xt and Wy\n",
    "    n_x, m = xt.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "\n",
    "    ### MAIN CODE HERE ###\n",
    "    # concatenate a_prev and xt\n",
    "    concat = np.zeros((n_a + n_x, m))\n",
    "    concat[:n_a,:] = a_prev\n",
    "    concat[n_a:,:] = xt\n",
    "    \n",
    "    # Compute values for ft, it, cct, c_next, ot, a_next using the formulas shown above\n",
    "    ft = sigmoid(np.dot(Wf,concat) +bf)\n",
    "    it = sigmoid(np.dot(Wi,concat) +bi)\n",
    "    cct = np.tanh(np.dot(Wo,concat) +bc)\n",
    "    c_next = ft*c_prev + it * cct\n",
    "    ot = sigmoid(np.dot(Wf,concat) +bf)\n",
    "    a_next = ot * np.tanh(c_next)\n",
    "    \n",
    "    # Compute the prediction of the LSTM cell\n",
    "    yt_pred = torch.nn.Softmax(np.dot(Wy,a_next) +by)\n",
    "    yt_pred =  yt_pred.dim\n",
    "    ### MAIN ENDS HERE ###\n",
    "    \n",
    "    # store values needed for backward propagation in cache\n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
    "\n",
    "    return a_next, c_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T09:04:05.409830Z",
     "start_time": "2020-12-21T09:04:05.381885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_next[4] =  [ 6.50250148e-01 -2.74012030e-04 -1.74184734e-04  3.14518225e-02\n",
      " -5.77828015e-01 -3.23325131e-02  4.35000746e-01  1.98681487e-01\n",
      " -5.99909235e-01  6.14818200e-01]\n",
      "a_next.shape =  (5, 10)\n",
      "c_next[2] =  [ 0.57207884  1.35755352  0.44439288  0.14919283 -1.54046493  0.57356263\n",
      "  0.01422994 -1.78940624 -0.97897515 -0.55744859]\n",
      "c_next.shape =  (5, 10)\n",
      "yt[1] = [-0.97273119 -0.96198061 -1.58100467 -1.441182    0.34621894 -0.36082323\n",
      " -1.07411669 -0.393978    0.04028089 -1.47979389]\n",
      "yt.shape =  (2, 10)\n",
      "cache[1][3] = [-0.17802135  0.10394883  0.58336635  0.46341066  0.01029736 -0.83420601\n",
      " -0.07906531 -0.34425587 -0.25242088  1.1182707 ]\n",
      "len(cache) =  10\n"
     ]
    }
   ],
   "source": [
    "m = 10 # number_of_example\n",
    "input_feature_size = 3 # each word could be represented to a vector of size 3\n",
    "hidden_size = 5 # hidden layer dimensions\n",
    "output_size = 2 # how many output you want\n",
    "\n",
    "\n",
    "xt = np.random.randn(input_feature_size,m)\n",
    "a_prev = np.random.randn(hidden_size,m)\n",
    "c_prev = np.random.randn(hidden_size,m)\n",
    "Wf = np.random.randn(hidden_size,input_feature_size+hidden_size)\n",
    "bf = np.random.randn(hidden_size,1)\n",
    "Wi = np.random.randn(hidden_size,input_feature_size+hidden_size)\n",
    "bi = np.random.randn(hidden_size,1)\n",
    "Wo = np.random.randn(hidden_size,input_feature_size+hidden_size)\n",
    "bo = np.random.randn(hidden_size,1)\n",
    "Wc = np.random.randn(hidden_size,input_feature_size+hidden_size)\n",
    "bc = np.random.randn(hidden_size,1)\n",
    "Wy = np.random.randn(output_size,hidden_size)\n",
    "by = np.random.randn(output_size,1)\n",
    "\n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "\n",
    "a_next, c_next, yt, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)\n",
    "print(\"a_next[4] = \", a_next[4])\n",
    "print(\"a_next.shape = \", c_next.shape)\n",
    "print(\"c_next[2] = \", c_next[2])\n",
    "print(\"c_next.shape = \", c_next.shape)\n",
    "print(\"yt[1] =\", yt[1])\n",
    "print(\"yt.shape = \", yt.shape)\n",
    "print(\"cache[1][3] =\", cache[1][3])\n",
    "print(\"len(cache) = \", len(cache))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T07:52:01.077620Z",
     "start_time": "2020-12-21T07:52:01.071419Z"
    }
   },
   "source": [
    "# LSTM FULL FORWARD PROPAGATION FULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T07:52:35.979182Z",
     "start_time": "2020-12-21T07:52:35.867122Z"
    }
   },
   "source": [
    "![LSTM Cell](./images/lstm_unroll.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T09:04:18.964387Z",
     "start_time": "2020-12-21T09:04:18.948782Z"
    }
   },
   "outputs": [],
   "source": [
    "def lstm_forward(x,a0,parameters):\n",
    "    \"\"\"\n",
    "    Impliments forward propagation of LSTM-cell unrolled in time as described in the fig above\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Input data for all the timesteps, numpy array of shape(n_x,m,T_x)\n",
    "    a0 -- Initial hidden state, of shape(n_a,m)\n",
    "    \n",
    "    parameters -- python dict containing:\n",
    "                    Wf -- Weights matrix of the forgate gate, numpy array of shape(n_a,n_a+n_x)\n",
    "                    bf -- Bias of the forgate gate, numpy array of shape (n_a,1)\n",
    "                    Wi -- Weights matrix of the update gate, numpy array of shape(n_a,n_a+n_x)\n",
    "                    bi -- Bias of the update gate, numpy array of shape (n_a,1)\n",
    "                    Wc -- Weights matrix of the first \"tanh\", numpy array of shape(n_a,n_a+n_x)\n",
    "                    bc -- Bias of the first \"tanh\", numpy array of shape (n_a,1)\n",
    "                    Wo -- Weights matrix of the output gate, numpy array of shape(n_a,n_a+n_x)\n",
    "                    bo -- Bias of the output gate, numpy array of shape (n_a,1)\n",
    "                    Wf -- Weights matrix relating the hidden state to the output, numpy array of shape(n_y,n_a)\n",
    "                    bf -- Bias relating the hidden state to the output, numpy array of shape (n_y,1)\n",
    "    Returns:\n",
    "    a -- hidden state for every timestep, of shape(n_a,m,T_x)\n",
    "    y -- Predictions for every time-step numpy array of shape (n_a,m,T_x)\n",
    "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y,m)\n",
    "    cache -- tuple of values needed for the backward pass, contains (list of all caches,x)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize \"caches\" which will track the list of all the caches\n",
    "    caches = []\n",
    "    \n",
    "    ### MAIN CODE HERE ###\n",
    "    # Retrieve dimensions from shapes of x and Wy\n",
    "    n_x,m,T_x = x.shape\n",
    "    n_y,n_a = parameters[\"Wy\"].shape\n",
    "    \n",
    "    # Initialize \"a\", \"c\" and \"y\" with zeros\n",
    "    a = np.zeros((n_a,m,T_x))\n",
    "    c = a\n",
    "    y = np.zeros((n_y,m,T_x))\n",
    "    \n",
    "    # Initialize a_next and c_next \n",
    "    a_next = a0 # initial state of hidden vector\n",
    "    c_next = np.zeros(a_next.shape) # initial state of context vector\n",
    "    \n",
    "    # Loop over all time-steps\n",
    "    for t in range(T_x):\n",
    "        # Update next hidden state, next memory state, compute the prediction, get the cahce\n",
    "        a_next, c_next, yt, cache = lstm_cell_forward(x[:,:,t],a_next,c_next,parameters)\n",
    "        # save the values of the new \"next\" hidden state in a\n",
    "        a[:,:,t] = a_next\n",
    "        # save the values of pred\n",
    "        y[:,:,t] = yt\n",
    "        # save the values of the next cell state\n",
    "        c[:,:,t] = c_next\n",
    "        # Append the cache inot caches\n",
    "        caches.append(cache)\n",
    "    ### MAIN ENDS HERE ###\n",
    "    \n",
    "    #store values needed for the back propagation in cache\n",
    "    cahces = (caches,x)\n",
    "    \n",
    "    return a,y,c,cahces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T09:04:19.517810Z",
     "start_time": "2020-12-21T09:04:19.507757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a[4][3][6] =  0.21437420411361274\n",
      "a.shape =  (5, 10, 7)\n",
      "y[1][4][3] = -0.2015587769196316\n",
      "y.shape =  (2, 10, 7)\n",
      "caches[1][1[1]] = [ 2.11060505 -1.30653407  0.07638048  0.36723181  1.23289919 -0.42285696\n",
      "  0.08646441]\n",
      "c[1][2][1] 0.04794387049025145\n",
      "len(caches) =  2\n"
     ]
    }
   ],
   "source": [
    "m = 10 # number_of_example\n",
    "input_feature_size = 3 # each word could be represented to a vector of size 3\n",
    "hidden_size = 5 # hidden layer dimensions\n",
    "output_size = 2 # how many output you want\n",
    "time_step_size = 7 # time steps\n",
    "\n",
    "\n",
    "x = np.random.randn(input_feature_size,m,time_step_size)\n",
    "a0 = np.random.randn(hidden_size,m)\n",
    "Wf = np.random.randn(hidden_size,input_feature_size+hidden_size)\n",
    "bf = np.random.randn(hidden_size,1)\n",
    "Wi = np.random.randn(hidden_size,input_feature_size+hidden_size)\n",
    "bi = np.random.randn(hidden_size,1)\n",
    "Wo = np.random.randn(hidden_size,input_feature_size+hidden_size)\n",
    "bo = np.random.randn(hidden_size,1)\n",
    "Wc = np.random.randn(hidden_size,input_feature_size+hidden_size)\n",
    "bc = np.random.randn(hidden_size,1)\n",
    "Wy = np.random.randn(output_size,hidden_size)\n",
    "by = np.random.randn(output_size,1)\n",
    "\n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "\n",
    "a, y, c, caches = lstm_forward(x, a0, parameters)\n",
    "print(\"a[4][3][6] = \", a[4][3][6])\n",
    "print(\"a.shape = \", a.shape)\n",
    "print(\"y[1][4][3] =\", y[1][4][3])\n",
    "print(\"y.shape = \", y.shape)\n",
    "print(\"caches[1][1[1]] =\", caches[1][1][1])\n",
    "print(\"c[1][2][1]\", c[1][2][1])\n",
    "print(\"len(caches) = \", len(caches))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
